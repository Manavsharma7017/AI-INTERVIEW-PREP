# 🧠 AI-INTERVIEW-PREP

This project is a **multi-service web application** combining Go, Python, and React to provide a robust platform for Large Language Model (LLM) processing.

It features:

- A Go backend server that exposes REST APIs.
- A Python gRPC server (and HTTP fallback server) to handle LLM calls.
- A React frontend for user interaction.
- Docker Compose for easy orchestration.

---

## 📂 Project Structure

.
├── backend/ # Go HTTP server handling client requests
│ ├── main.go
│ ├── go.mod
│ └── ...
├── llmserver/ # Python gRPC server for LLM calls
│ ├── main.py
│ ├── requirements.txt
│ └── ...
├── llmhttpserver/ # Python HTTP server for LLM calls (fallback for deployment)
│ ├── main.py
│ ├── requirements.txt
│ └── ...
├── frontend/ # React + TypeScript frontend
│ ├── src/
│ ├── package.json
│ └── ...
├── Proto/ #grpc proto file
│ ├── a.proto
├── docker-compose.yml
└── README.md
└── Makefile


---

## ✨ Overview

### Backend (Go HTTP Server)

- Accepts HTTPS requests from clients.
- Validates and processes incoming data.
- Forwards requests to the Python LLM server over **gRPC** (preferred) or **HTTP** (fallback).
- Returns processed responses to the frontend.

### LLM Server (Python)

- **gRPC Server (`llmserver/`)**:
  - Provides efficient APIs for LLM processing.
  - Used in development and any environments that support gRPC.
- **HTTP Server (`llmhttpserver/`)**:
  - Provides REST endpoints equivalent to the gRPC server.
  - Used in production deployments where gRPC is not available.

### Frontend (React + TypeScript)

- Interactive UI for users.
- Sends REST API requests to the Go backend.
- Displays responses generated by the LLM server.

### Proto 
- Run Makefile with make gen for go and make gen2 for python
---

## 🐳 Running with Docker Compose

This project uses Docker Compose to build and run all services together.

### Build and Start All Services

## 🐳 Running with Docker Compose

To build and start **all services**, run:

```bash
docker-compose up --build
Docker Compose will:

✅ Build Docker images for:

Go backend server

Python gRPC server 

Python HTTP server

✅ Create a Docker network so services can reach each other

✅ Expose the relevant ports to your host machine

⚙️ Development Setup
You can also run services individually for development.

Start the Go Backend
bash
Copy
Edit
cd backend
go run main.go
Start the Python LLM Server for grpc uncomment the feedback-service.go and comment http version
Option 1: gRPC Server

bash
Copy
Edit
cd llmserver
python server.py
Option 2: HTTP Server

bash
Copy
Edit
cd llmhttpserver
python main.py
Note: Run only one server at a time.

Start the Frontend
bash
Copy
Edit
cd frontend
npm install
npm run dev
The frontend will be available at:

arduino
Copy
Edit
http://localhost:5173
🌍 API Endpoints
Go Backend (default ports)

http://localhost:3000/api/... – REST APIs exposed to the frontend

LLM gRPC Server

localhost:50051 – gRPC service endpoint

LLM HTTP Server

http://localhost:80/submit/... – HTTP fallback endpoint